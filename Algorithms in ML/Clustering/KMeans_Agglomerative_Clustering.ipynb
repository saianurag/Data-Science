{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **KMeans Clustering**\n",
    "- As we discussed previously, the ML algorithms can be broadly classified into two categories: `Supervised` and `Unsupervised`\n",
    "    - When we have a \"target\" and we need to obtain a function \"f\" with independent variables \"X\"s to predict target, the approach is called `Supervised`.\n",
    "    - When we do not have a target, in such cases we try to understand the data by looking at the relations among the instances wrt the variables given (which instances are similar or what is relation between instance a and instance b etc). Today we will be learning another algorithm called kmeans\n",
    "- Kmeans is a distance based iterative technique, where the instances that are \"closer\" are \"grouped\" together forming a \"`cluster`\". \n",
    "- This \"closeness\" is computed by distances,by default, Euclidean distances\n",
    "- We need to specify prior, how many clusters we want to get.\n",
    "- What is iterative in this case?\n",
    "    - We specify a number of clusters we need, so in the first iteration, centroids(centre) of the cluster are randomly picked in the data (this centroid need not be a data point but could be any other point as well). For eg: if we need 3 clusters, 3 centroids are randomly picked.\n",
    "    - Now with respect to each of these centroids, distance is computed for each of the points in the data and the data point is assigned to that cluster for which the point's distance is closest to its centroid. This is \"Assignment phase\".\n",
    "    - Once all points are assigned to the clusters, new centroids are computed from the points of each cluster (in 2d it is (x1+x2)/2, (y1+y2)/2)..remember this formula :)\n",
    "    - Once, this new centroids are computed, the assignment phase starts-- compute the distance between each of the data points with each of the new centroids and assign the point to the closest cluster. After assignment, the new cluster centroids are computed. This process continues until there is no change in cluster centroids from previous iteration\n",
    "    \n",
    "    \n",
    " - **Getting Ideal K**\n",
    "    - we find the clusters by assessing the `within sum of squares of distances (wss)` of all points to the centroid of cluster.\n",
    "    - We experiment with k, starting with 1 (number of clusters formed would be 1 in first iteration and wss is computed, then 2 clusters are created and wss is computed, then 3 clusters and so on).\n",
    "    - We plot this wss with the k (k value on x-axis and wss on y-axis).This plot is known as a scree plot. We inspect this plot to get the ideal k.\n",
    "    - Other measures that are used to check if the clustering is proper are silhouette distances.\n",
    "\n",
    " - **Clustering Stability**\n",
    "    - Once the data points are segmented into clusters, we need to check for cluster stability i.e. if we run the same algorithm, will the data points that were segmented into one cluster previously are segmented go together again. If yes, then the cluster is stable else not.\n",
    "    \n",
    "- **Interpretation of centroids**\n",
    "    - Centroids of the clusters are like representative of the cluster. For example, if we are segmenting customers of retail stores based on certain attributes like average spending, items purchased, preferred mode of transaction etc.,then the centroid of a cluster would represent (on an average for each of the attribute) the behaviour of customer in that cluster. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Note on Distances*\n",
    "\n",
    "- In an ideal case, when all the variables are numeric, Euclidean distance makes sense\n",
    "(If we want to use Euclidean).\n",
    "- Other distance measures we have are- for numeric we have **Euclidean, Manhattan,Chebyshev** and for categorical we have **Hamming, Jaccard etc**\n",
    "- If the data has mixed variables (both numeric and categorical) then a *quick way to*\n",
    "*do clustering is by creating dummy variables for categorical attributes*. Even when using Euclidean as a distance metric, this approach may not be appreciated by some\n",
    "Data Science practitioners.Another approach for dealing with mixed variables is to\n",
    "use `Gower’s algorithm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.091493Z",
     "start_time": "2020-07-19T10:46:07.086507Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.134379Z",
     "start_time": "2020-07-19T10:46:07.100467Z"
    }
   },
   "outputs": [],
   "source": [
    "cereals = # read the dataset\n",
    "cereals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "- `calories`: calories per serving\n",
    "- `protein`: grams of protein\n",
    "- `fat`: grams of fat\n",
    "- `sodium`: milligrams of sodium\n",
    "- `fiber`: grams of dietary fiber\n",
    "- `carbo`: grams of complex carbohydrates\n",
    "- `sugars`: grams of sugars\n",
    "- `potass`: milligrams of potassium\n",
    "- `vitamins`: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended\n",
    "- `shelf`: display shelf (1, 2, or 3, counting from the floor)\n",
    "- `weight`: weight in ounces of one serving\n",
    "- `cups`: number of cups in one serving\n",
    "- `rating`: a rating of the cereals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping less relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.171279Z",
     "start_time": "2020-07-19T10:46:07.138367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the columns ['shelf','weight','cups','rating'] from the dataframe.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.244083Z",
     "start_time": "2020-07-19T10:46:07.176269Z"
    }
   },
   "outputs": [],
   "source": [
    "cereals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.258048Z",
     "start_time": "2020-07-19T10:46:07.247077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for Missing Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoupling name label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.269018Z",
     "start_time": "2020-07-19T10:46:07.261041Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = cereals['name']\n",
    "cereals.drop(['name'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the missing values in the dataframe using SimpleImputer and strategy='mean'.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.305918Z",
     "start_time": "2020-07-19T10:46:07.292956Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using StandardScaler, perform standardization on the dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.400665Z",
     "start_time": "2020-07-19T10:46:07.311902Z"
    }
   },
   "outputs": [],
   "source": [
    "cereals_std.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "**Parameter description**\n",
    "\n",
    "`n_clusters` : The number of clusters to find.\n",
    "\n",
    "`linkage` : {“ward”, “complete”, “average”}\n",
    "\n",
    "**ward** minimizes the variance of the clusters being merged.\n",
    "\n",
    "**complete** uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "**average** uses the average of the distances of each observation of the two sets.\n",
    "\n",
    "**affinity** : {“euclidean”, “l1”, “l2”, “manhattan”, “cosine”}\n",
    "\n",
    "Metric used to compute the linkage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:07.438564Z",
     "start_time": "2020-07-19T10:46:07.404656Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "linkage_matrix = linkage(cereals_std, method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = range(1, cereals_std.shape[0]+1 )\n",
    "plt.figure(figsize=(12, 7)) \n",
    "dendrogram(linkage_matrix, labels=labelList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:15.801433Z",
     "start_time": "2020-07-19T10:46:10.377838Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clust = AgglomerativeClustering(n_clusters=6,\n",
    "                                affinity='euclidean',\n",
    "                                linkage='ward')\n",
    "\n",
    "cluster_predictions = clust.fit_predict(cereals)\n",
    "\n",
    "result = pd.DataFrame({'Label':labels,\n",
    "                       'Cluster':cluster_predictions})\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "**Parameter description**\n",
    "\n",
    "`n_clusters` : The number of clusters to find.\n",
    "\n",
    "`n_init` : Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "\n",
    "`max_iter` : max iterations of recomputing new cluster centroids\n",
    "\n",
    "`n_jobs` : The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:16.262349Z",
     "start_time": "2020-07-19T10:46:15.803428Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wss = []\n",
    "for k in range(2,15):\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(cereals_std)\n",
    "    wss.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:16.420642Z",
     "start_time": "2020-07-19T10:46:16.265056Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(range(2,15), wss, 'bx--')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('wss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans with 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:16.591694Z",
     "start_time": "2020-07-19T10:46:16.422638Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=6, random_state=4545, n_init=50)\n",
    "km.fit(cereals_std)\n",
    "kmeans_clusters = km.predict(cereals_std)\n",
    "\n",
    "result = pd.DataFrame({\"Label\":labels, \"KMeans Cluster\":kmeans_clusters})\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:46:16.629495Z",
     "start_time": "2020-07-19T10:46:16.593392Z"
    }
   },
   "outputs": [],
   "source": [
    "cereals_org = pd.read_csv(\"/home/divyas/Lab/Cereals.csv\")\n",
    "cereals_org.drop(['shelf','weight','cups','rating'], axis=1, inplace=True)\n",
    "cereals_org['Cluster'] = result['KMeans Cluster']\n",
    "cereals_org.groupby(\"Cluster\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking cluster stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=cereals_std.sample(frac=0.9,random_state=123).index\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals_std_subset=cereals_std.iloc[indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_object = KMeans(n_clusters=5,n_init=30,max_iter=300,random_state=1000)\n",
    "kmeans_object.fit(cereals_std)\n",
    "clus1= kmeans_object.predict(cereals_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_object = KMeans(n_clusters=5,n_init=30,max_iter=300,random_state=1000)\n",
    "kmeans_object.fit(cereals_std_subset)\n",
    "clus2= kmeans_object.predict(cereals_std_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clus1))\n",
    "print(len(clus2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus1=clus1[indices]\n",
    "print(len(clus1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(clus1,clus2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
